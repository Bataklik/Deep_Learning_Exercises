{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3ac0b0a",
   "metadata": {},
   "source": [
    "# 3  Translation with an Encoder-Decoder Trans-former\n",
    "In this exercise, we will build an encoder-decoder transformer and apply it to the“translation” of dates from one format to another format.The architecture we will build closely follows the seminal “Attention is All You\n",
    "\n",
    "## 3.1  Copy Layers from the Previous Exercise\n",
    "In the previous exercise on classification with an encoder you implemented\n",
    "  - FeedForward\n",
    "  - EmbeddingWithPosition\n",
    "  - EncoderBlockAll these \n",
    "layers are identical in the encoder-decoder architecture we will build.Copy these layers to the current file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88bae409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras) (1.4.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras) (2.0.2)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras) (0.1.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras) (3.15.1)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras) (0.18.0)\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras) (0.5.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras) (25.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras) (4.15.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.11.12)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "%pip install keras\n",
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f664161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b809e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple FeedForward layer\n",
    "@keras.saving.register_keras_serializable()\n",
    "class FeedForward(keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, factor=4, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.factor = factor\n",
    "\n",
    "    def build(self, batch_input_shape):\n",
    "        time_steps, embed_size = batch_input_shape[1:]\n",
    "        #! YOUR CODE HERE:\n",
    "        self.w1 = self.add_weight(shape=(embed_size, self.factor * embed_size))\n",
    "        self.w2 = self.add_weight(shape=(self.factor * embed_size, embed_size))\n",
    "        self.b1 = self.add_weight(shape=(self.factor * embed_size,))\n",
    "        self.b2 = self.add_weight(shape=(embed_size,))\n",
    "\n",
    "    #? Call kun je oproepen met `FeedForward()()`\n",
    "    def call(self, inputs):\n",
    "        #! YOUR CODE HERE:\n",
    "        #! Perform calculation on inputs and return result\n",
    "        inputs = keras.ops.matmul(inputs,self.w1)\n",
    "        inputs = inputs + self.b1\n",
    "        inputs = keras.layers.Activation(\"relu\")(inputs)\n",
    "\n",
    "        inputs = keras.ops.matmul(inputs,self.w2)\n",
    "        inputs = inputs + self.b2\n",
    "        return inputs\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return{**base_config,\"factor\": self.factor,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c10ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class EmbeddingWithPosition(keras.layers.Layer):\n",
    "    def __init__(self, num_tokens, max_seq_length, embed_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        #! YOUR CODE HERE\n",
    "        #! Save constructor arguments\n",
    "        self.num_tokens = num_tokens\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.embed_size = embed_size\n",
    "    def build(self, batch_input_shape):\n",
    "        print(f\"Building EmbeddingWithPosition with input shape {batch_input_shape}\")\n",
    "        #! Shape not actually needed!!\n",
    "        #! YOUR CODE HERE\n",
    "        #! Add the weights for the two embeddings\n",
    "        #? Token kunnen omzetten naar een embedding?\n",
    "        #? token 0 (the)\n",
    "        #? --> embedding [30,45,29,..., 223,45] # 512\n",
    "        #? token 2 (or)\n",
    "        #? --> embedding [12,34,56,...,78] # 512\n",
    "        #? [\n",
    "        #? (0): [30,45,29,...,223,45],\n",
    "        #? ...\n",
    "        #? (2): [12,34,56,...,78]\n",
    "        #? ]\n",
    "        self.embedding_loop_table = self.add_weight(shape=(self.num_tokens,self.embed_size))\n",
    "        self.position_lookup_table = self.add_weight(shape=(self.max_seq_length,self.embed_size))\n",
    "    def call(self, inputs):\n",
    "        _, length = keras.ops.shape(inputs)\n",
    "        #? YOUR CODE HERE\n",
    "        #? Get both embeddings and add them.\n",
    "        token_embeddings = keras.ops.take(self.embedding_loop_table,inputs,axis=0)\n",
    "        position_embeddings = self.position_lookup_table[:length]\n",
    "        return token_embeddings + position_embeddings\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return{**base_config,\n",
    "                \"num_tokens\": self.num_tokens,\n",
    "                \"max_seq_length\": self.max_seq_length,\n",
    "                \"embed_size\": self.embed_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdce9a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class EncoderBlock(keras.layers.Layer):\n",
    "    def __init__(self, num_heads, embed_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # Self-Attention\n",
    "        self.attention = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=(embed_size // num_heads), name=\"self_attention\"\n",
    "        )\n",
    "        self.norm_1 = keras.layers.LayerNormalization(name=\"norm_1\")\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        self.feed_forward = FeedForward(name=\"feed_forward\")\n",
    "        self.norm_2 = keras.layers.LayerNormalization(name=\"norm_2\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 1. Self-Attention + Skip Connection + Normalization\n",
    "        skip_1 = inputs\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, use_causal_mask=False # Encoder is niet-causaal\n",
    "        )\n",
    "        # Correctie: Keras ops.add in plaats van keras.ops.add()[...]\n",
    "        x = self.norm_1(keras.layers.add([attention_output, skip_1]))\n",
    "\n",
    "        # 2. Feed-Forward + Skip Connection + Normalization\n",
    "        skip_2 = x\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm_2(keras.layers.add([ff_output, skip_2]))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return {**config, \"num_heads\": self.num_heads, \"embed_size\": self.embed_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df0d13e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class DecoderBlock(keras.layers.Layer):\n",
    "    def __init__(self, num_heads, embed_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        # 1. Causal Self-Attention\n",
    "        self.causal_attention = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=(embed_size // num_heads), name=\"causal_attention\"\n",
    "        )\n",
    "        self.norm_1 = keras.layers.LayerNormalization(name=\"norm_1\")\n",
    "\n",
    "        # 2. Cross-Attention (Encoder-Decoder Attention)\n",
    "        self.cross_attention = keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=(embed_size // num_heads), name=\"cross_attention\"\n",
    "        )\n",
    "        self.norm_2 = keras.layers.LayerNormalization(name=\"norm_2\")\n",
    "\n",
    "        # 3. Feed-Forward Network\n",
    "        self.feed_forward = FeedForward(name=\"feed_forward\")\n",
    "        self.norm_3 = keras.layers.LayerNormalization(name=\"norm_3\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs is een lijst: [decoder_embs, encoder_embs]\n",
    "        decoder_embs, encoder_embs = inputs[0], inputs[1]\n",
    "\n",
    "        # 1. Causal Self-Attention + Skip Connection + Normalization\n",
    "        skip_1 = decoder_embs\n",
    "        ca_output = self.causal_attention(\n",
    "            query=decoder_embs, value=decoder_embs, key=decoder_embs, use_causal_mask=True\n",
    "        )\n",
    "        x = self.norm_1(keras.layers.add([ca_output, skip_1]))\n",
    "\n",
    "        # 2. Cross-Attention + Skip Connection + Normalization\n",
    "        # Queries (Q) komen van de vorige decoderlaag (x)\n",
    "        # Keys (K) en Values (V) komen van de encoder output (encoder_embs)\n",
    "        skip_2 = x\n",
    "        cross_output = self.cross_attention(\n",
    "            query=x, key=encoder_embs, value=encoder_embs\n",
    "        )\n",
    "        x = self.norm_2(keras.layers.add([cross_output, skip_2]))\n",
    "\n",
    "        # 3. Feed-Forward + Skip Connection + Normalization\n",
    "        skip_3 = x\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm_3(keras.layers.add([ff_output, skip_3]))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return {**config, \"num_heads\": self.num_heads, \"embed_size\": self.embed_size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d798b38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building EmbeddingWithPosition with input shape (None, None)\n",
      "Building EmbeddingWithPosition with input shape (None, None)\n"
     ]
    }
   ],
   "source": [
    "def get_encoder_decoder_model(\n",
    "    num_tokens_enc: int, max_seq_length_enc: int,\n",
    "    num_tokens_dec: int, max_seq_length_dec: int,\n",
    "    embed_size: int, num_heads: int, num_blocks: int,\n",
    "    use_mask: bool = False # De maskering logica is verwijderd voor eenvoud\n",
    ") -> keras.Model:\n",
    "\n",
    "    # --- De Encoder ---\n",
    "    encoder_input = keras.layers.Input(shape=(None,),\n",
    "                                       dtype=tf.int32, name=\"encoder_input\")\n",
    "\n",
    "    # Positional embedding (met de gecorrigeerde klasse naam)\n",
    "    encoder_embs = EmbeddingWithPosition(\n",
    "        num_tokens=num_tokens_enc,\n",
    "        max_seq_length=max_seq_length_enc,\n",
    "        embed_size=embed_size,\n",
    "        name=\"enc_positional_embedding\"\n",
    "    )(encoder_input)\n",
    "\n",
    "    # Encoder blocks\n",
    "    encoder_output = encoder_embs\n",
    "    for index in range(num_blocks):\n",
    "        # EncoderBlock ontvangt slechts één input: de embeddings\n",
    "        encoder_output = EncoderBlock(\n",
    "            num_heads=num_heads,\n",
    "            embed_size=embed_size,\n",
    "            name=f\"encoder_block_{index}\"\n",
    "        )(encoder_output)\n",
    "\n",
    "    # --- De Decoder ---\n",
    "    decoder_input = keras.layers.Input(shape=(None,),\n",
    "                                       dtype=tf.int32, name=\"decoder_input\")\n",
    "\n",
    "    # Positional embedding\n",
    "    decoder_embs = EmbeddingWithPosition(\n",
    "        num_tokens=num_tokens_dec,\n",
    "        max_seq_length=max_seq_length_dec,\n",
    "        embed_size=embed_size,\n",
    "        name=\"dec_positional_embedding\"\n",
    "    )(decoder_input)\n",
    "\n",
    "    # Decoder blocks\n",
    "    decoder_output = decoder_embs\n",
    "    for index in range(num_blocks):\n",
    "        # DecoderBlock ontvangt een lijst: [decoder_embeddings, encoder_embeddings]\n",
    "        decoder_output = DecoderBlock(\n",
    "            num_heads=num_heads,\n",
    "            embed_size=embed_size,\n",
    "            name=f\"decoder_block_{index}\"\n",
    "        )([decoder_output, encoder_output]) # encoder_output bevat de finale embeddings\n",
    "\n",
    "    # Classification head. Output logits, niet softmax\n",
    "    decoder_output = keras.layers.Dense(\n",
    "        units=num_tokens_dec, activation=\"linear\", name=\"output_logits\"\n",
    "    )(decoder_output)\n",
    "\n",
    "    return keras.Model(inputs=[encoder_input, decoder_input],\n",
    "                       outputs=decoder_output, name=\"Transformer_Encoder_Decoder\")\n",
    "\n",
    "# --- Test de Modelinitialisatie ---\n",
    "\n",
    "# Noot: num_tokens_dec moet 13 zijn voor de latere opgave, maar we gebruiken 10 voor de check.\n",
    "model = get_encoder_decoder_model(\n",
    "    num_tokens_enc=20, max_seq_length_enc=30,\n",
    "    num_tokens_dec=10, max_seq_length_dec=12,\n",
    "    embed_size=32, num_heads=2, num_blocks=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ee22aa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Transformer_Encoder_Decoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Transformer_Encoder_Decoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_positional_emb… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,600</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EmbeddingWithPosi…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_block_0     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,704</span> │ enc_positional_e… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderBlock</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_block_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,704</span> │ encoder_block_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderBlock</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_block_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,704</span> │ encoder_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderBlock</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_positional_emb… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EmbeddingWithPosi…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_block_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,704</span> │ encoder_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderBlock</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_block_0     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,992</span> │ dec_positional_e… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │                   │            │ encoder_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_block_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,992</span> │ decoder_block_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │                   │            │ encoder_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_block_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,992</span> │ decoder_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │                   │            │ encoder_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_block_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">16,992</span> │ decoder_block_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │                   │            │ encoder_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_logits       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │ decoder_block_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_positional_emb… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │      \u001b[38;5;34m1,600\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbeddingWithPosi…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_block_0     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │     \u001b[38;5;34m12,704\u001b[0m │ enc_positional_e… │\n",
       "│ (\u001b[38;5;33mEncoderBlock\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_block_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │     \u001b[38;5;34m12,704\u001b[0m │ encoder_block_0[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mEncoderBlock\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_block_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │     \u001b[38;5;34m12,704\u001b[0m │ encoder_block_1[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mEncoderBlock\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_positional_emb… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │        \u001b[38;5;34m704\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbeddingWithPosi…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_block_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │     \u001b[38;5;34m12,704\u001b[0m │ encoder_block_2[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mEncoderBlock\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_block_0     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │     \u001b[38;5;34m16,992\u001b[0m │ dec_positional_e… │\n",
       "│ (\u001b[38;5;33mDecoderBlock\u001b[0m)      │                   │            │ encoder_block_3[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_block_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │     \u001b[38;5;34m16,992\u001b[0m │ decoder_block_0[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDecoderBlock\u001b[0m)      │                   │            │ encoder_block_3[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_block_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │     \u001b[38;5;34m16,992\u001b[0m │ decoder_block_1[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDecoderBlock\u001b[0m)      │                   │            │ encoder_block_3[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_block_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │     \u001b[38;5;34m16,992\u001b[0m │ decoder_block_2[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDecoderBlock\u001b[0m)      │                   │            │ encoder_block_3[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_logits       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)  │        \u001b[38;5;34m330\u001b[0m │ decoder_block_3[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">121,418</span> (474.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m121,418\u001b[0m (474.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">121,418</span> (474.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m121,418\u001b[0m (474.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of result: (2, 12, 10)\n",
      "Shape of result (shorter sequences): (2, 9, 10)\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "# Dit zou nu correct moeten draaien en het aantal parameters van 121.418 moeten opleveren.\n",
    "\n",
    "# --- Test de voorspelling (gecorrigeerd) ---\n",
    "# Fout: tf.random.inform bestaat niet. tf.random.uniform is correct.\n",
    "X_enc = tf.random.uniform(shape=(2,30), minval=0, maxval=20, dtype=tf.int32)\n",
    "X_dec = tf.random.uniform(shape=(2,12), minval=0, maxval=10, dtype=tf.int32)\n",
    "print(f\"Shape of result: {model([X_enc, X_dec]).shape}\")\n",
    "# Verwachte Shape: (2, 12, 10)\n",
    "\n",
    "X_enc_short = tf.random.uniform(shape=(2,15), minval=0, maxval=20, dtype=tf.int32)\n",
    "X_dec_short = tf.random.uniform(shape=(2,9), minval=0, maxval=10, dtype=tf.int32)\n",
    "print(f\"Shape of result (shorter sequences): {model([X_enc_short, X_dec_short]).shape}\")\n",
    "# Verwachte Shape: (2, 9, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53d2dd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\n",
      "  Downloading faker-38.2.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tzdata in /usr/local/lib/python3.12/dist-packages (from faker) (2025.2)\n",
      "Downloading faker-38.2.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faker\n",
      "Successfully installed faker-38.2.0\n",
      "Requirement already satisfied: babel in /usr/local/lib/python3.12/dist-packages (2.17.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install faker\n",
    "%pip install babel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdae3bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import TextVectorization\n",
    "import faker\n",
    "import random\n",
    "import babel.dates\n",
    "\n",
    "# --- Definitie van de Data-functies ---\n",
    "\n",
    "fake = faker.Faker()\n",
    "faker.Faker.seed(12345)\n",
    "random.seed(12345)\n",
    "\n",
    "FORMATS = [\n",
    "    'short', 'medium', 'long', 'full', 'full',\n",
    "    'full', 'full', 'full', 'full', 'full', 'full',\n",
    "    'full', 'full', 'd MMM YYY', 'd MMMM YYY',\n",
    "    'dd MMM YYY', 'd MMM, YYY', 'd MMMM, YYY',\n",
    "    'dd, MMM YYY', 'd MM YY', 'd MMMM YYY',\n",
    "    'MMMM d YYY', 'MMMM d, YYY', 'dd.MM.YY'\n",
    "]\n",
    "# Meer locales toevoegen om de encoder te helpen generaliseren\n",
    "LOCALES = ['nl_NL', 'de_DE', 'en_US', 'fr_FR', 'it_IT']\n",
    "\n",
    "def load_date():\n",
    "    dt = fake.date_object()\n",
    "    try:\n",
    "        human_readable = babel.dates.format_date(\n",
    "            dt,\n",
    "            format=random.choice(FORMATS),\n",
    "            locale=random.choice(LOCALES)\n",
    "        ).lower().replace(',', '')\n",
    "        machine_readable = dt.isoformat()\n",
    "    except (AttributeError, ValueError):\n",
    "        # Vang alle mogelijke fouten bij het genereren van datums\n",
    "        return None, None, None\n",
    "    return human_readable, machine_readable, dt\n",
    "\n",
    "def load_dataset(m_count): # De parameter is hernoemd naar m_count\n",
    "    dataset = []\n",
    "    i = 0\n",
    "    while i < m_count: # Vergelijking is nu tussen twee integers (i en m_count)\n",
    "        h, m_date, _ = load_date() # De machine-readable string wordt opgeslagen in m_date\n",
    "        if h is not None and m_date is not None:\n",
    "            dataset.append((h, m_date))\n",
    "            i += 1\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbbacea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.4.1 Dataset Creation ---\n",
    "\n",
    "dataset = load_dataset(20_000)\n",
    "\n",
    "train = dataset[:10_000]\n",
    "valid = dataset[10_000:15_000]\n",
    "test  = dataset[15_000:]\n",
    "\n",
    "# Gebruik zip om de data te splitsen in human-readable en machine-readable\n",
    "train_human, train_machine = zip(*train)\n",
    "valid_human, valid_machine = zip(*valid)\n",
    "test_human, test_machine = zip(*test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4963c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3.4.2 Create TextVectorization Layers ---\n",
    "\n",
    "# Encoder TextVectorization (leert van de 'human' data)\n",
    "enc_text_vec_layer = TextVectorization(\n",
    "    split=\"character\",\n",
    "    standardize=None # Voorkom automatische lowercasing/stripping\n",
    ")\n",
    "# Pas aan op de trainingsdata\n",
    "enc_text_vec_layer.adapt(list(train_human))\n",
    "\n",
    "# Decoder TextVectorization (vast vocabulaire)\n",
    "# *: End-of-sequence, .: Start-of-sequence\n",
    "decoder_vocabulary=[\"*\", \".\", \"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"-\"]\n",
    "num_tokens_dec_final = len(decoder_vocabulary) # Dit is 13\n",
    "\n",
    "dec_text_vec_layer = TextVectorization(\n",
    "    split=\"character\",\n",
    "    standardize=None,\n",
    "    vocabulary=decoder_vocabulary,\n",
    "    # Opmerking: TextVectorization voegt automatisch een '[UNK]' (index 0) en een '[PAD]' (index 1) toe\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bbcab36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_enc: (10000, 29)\n"
     ]
    }
   ],
   "source": [
    "# --- Creëer training data voor de encoder ---\n",
    "X_train_enc = enc_text_vec_layer(list(train_human))\n",
    "X_valid_enc = enc_text_vec_layer(list(valid_human))\n",
    "X_test_enc = enc_text_vec_layer(list(test_human))\n",
    "\n",
    "print(f\"Shape of X_train_enc: {X_train_enc.shape}\") # Moet (10000, 29) zijn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1877648d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_dec: (10000, 11)\n",
      "Shape of Y_train_dec: (10000, 11)\n"
     ]
    }
   ],
   "source": [
    "# --- Creëer inputs en targets voor de decoder ---\n",
    "\n",
    "# X_dec: Start-of-sequence ('.') gevolgd door de target datum. (e.g., '.1992-06-16')\n",
    "X_train_dec_str = [(\".\" + m) for m in train_machine]\n",
    "X_valid_dec_str = [(\".\" + m) for m in valid_machine]\n",
    "X_test_dec_str = [(\".\" + m) for m in test_machine]\n",
    "\n",
    "# Y_dec: De target datum gevolgd door End-of-sequence ('*'). (e.g., '1992-06-16*')\n",
    "Y_train_dec_str = [(m + \"*\") for m in train_machine]\n",
    "Y_valid_dec_str = [(m + \"*\") for m in valid_machine]\n",
    "Y_test_dec_str = [(m + \"*\") for m in test_machine]\n",
    "\n",
    "# Tokeniseren en 2 aftrekken:\n",
    "# De eerste twee indexen ([UNK]=0, [PAD]=1) worden genegeerd.\n",
    "# Onze tokens (*=2, .=3, 0=4, ...) krijgen nu nieuwe indexen (*=0, .=1, 0=2, ...)\n",
    "X_train_dec = dec_text_vec_layer(X_train_dec_str) - 2\n",
    "X_valid_dec = dec_text_vec_layer(X_valid_dec_str) - 2\n",
    "X_test_dec = dec_text_vec_layer(X_test_dec_str) - 2\n",
    "\n",
    "Y_train_dec = dec_text_vec_layer(Y_train_dec_str) - 2\n",
    "Y_valid_dec = dec_text_vec_layer(Y_valid_dec_str) - 2\n",
    "Y_test_dec = dec_text_vec_layer(Y_test_dec_str) - 2\n",
    "\n",
    "print(f\"Shape of X_train_dec: {X_train_dec.shape}\") # Moet (10000, 11) zijn\n",
    "print(f\"Shape of Y_train_dec: {Y_train_dec.shape}\") # Moet (10000, 11) zijn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6ef6d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huidige maximale sequence lengtes (op basis van de getokeniseerde data):\n",
    "# X_train_enc.shape[1] is de max lengte van de human-readable datum\n",
    "# X_train_dec.shape[1] is de max lengte van de machine-readable datum + '*' of '.'\n",
    "SEQ_LENGTH_ENC = X_train_enc.shape[1] # Ongeveer 29\n",
    "SEQ_LENGTH_DEC = X_train_dec.shape[1] # Ongeveer 11\n",
    "NUM_BLOCKS = 2\n",
    "NUM_HEADS = 2\n",
    "EMBED_SIZE = 16\n",
    "\n",
    "# Vocabulaire groottes van de TextVectorization layers:\n",
    "NUM_TOKENS_ENC = enc_text_vec_layer.vocabulary_size()\n",
    "# Decoder tokens zijn 13 (0 tot 12). Onze verschoven labels (Y_train_dec) gaan van 0 t/m 12.\n",
    "# Het uitvoer-vocabulaire is 13 (gelijk aan num_tokens_dec_final).\n",
    "NUM_TOKENS_DEC = num_tokens_dec_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a9768ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building EmbeddingWithPosition with input shape (None, None)\n",
      "Building EmbeddingWithPosition with input shape (None, None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Transformer_Encoder_Decoder\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Transformer_Encoder_Decoder\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_positional_emb… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,168</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EmbeddingWithPosi…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_block_0     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,280</span> │ enc_positional_e… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderBlock</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_positional_emb… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EmbeddingWithPosi…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_block_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,280</span> │ encoder_block_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EncoderBlock</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_block_0     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,400</span> │ dec_positional_e… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │                   │            │ encoder_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_block_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,400</span> │ decoder_block_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DecoderBlock</span>)      │                   │            │ encoder_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_logits       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">221</span> │ decoder_block_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ enc_positional_emb… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)  │      \u001b[38;5;34m1,168\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbeddingWithPosi…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_input       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_block_0     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)  │      \u001b[38;5;34m3,280\u001b[0m │ enc_positional_e… │\n",
       "│ (\u001b[38;5;33mEncoderBlock\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dec_positional_emb… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)  │        \u001b[38;5;34m384\u001b[0m │ decoder_input[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mEmbeddingWithPosi…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ encoder_block_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)  │      \u001b[38;5;34m3,280\u001b[0m │ encoder_block_0[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mEncoderBlock\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_block_0     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)  │      \u001b[38;5;34m4,400\u001b[0m │ dec_positional_e… │\n",
       "│ (\u001b[38;5;33mDecoderBlock\u001b[0m)      │                   │            │ encoder_block_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_block_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)  │      \u001b[38;5;34m4,400\u001b[0m │ decoder_block_0[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDecoderBlock\u001b[0m)      │                   │            │ encoder_block_1[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_logits       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)  │        \u001b[38;5;34m221\u001b[0m │ decoder_block_1[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,133</span> (66.93 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m17,133\u001b[0m (66.93 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">17,133</span> (66.93 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m17,133\u001b[0m (66.93 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Maak het model aan\n",
    "model_no_mask = get_encoder_decoder_model(\n",
    "    num_tokens_enc=NUM_TOKENS_ENC,\n",
    "    max_seq_length_enc=SEQ_LENGTH_ENC,\n",
    "    num_tokens_dec=NUM_TOKENS_DEC,\n",
    "    max_seq_length_dec=SEQ_LENGTH_DEC,\n",
    "    embed_size=EMBED_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    num_blocks=NUM_BLOCKS,\n",
    "    use_mask=False # We trainen zonder expliciete padding maskering\n",
    ")\n",
    "\n",
    "# Toon de samenvatting en controleer het aantal parameters\n",
    "model_no_mask.summary()\n",
    "\n",
    "# Het totaal aantal parameters zou 17,053 moeten zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d39d1f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definieer de Adam optimizer\n",
    "optimizer = keras.optimizers.Adam()\n",
    "\n",
    "# Definieer de loss functie (Logits output vereist from_logits=True)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Definieer de metriek (accuracy op token-niveau)\n",
    "metrics = [keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")]\n",
    "\n",
    "# Compileer het model\n",
    "model_no_mask.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=loss_fn,\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18853225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 51ms/step - accuracy: 0.9964 - loss: 0.0139 - val_accuracy: 0.9974 - val_loss: 0.0127\n",
      "Epoch 2/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 47ms/step - accuracy: 0.9975 - loss: 0.0118 - val_accuracy: 0.9973 - val_loss: 0.0123\n",
      "Epoch 3/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 54ms/step - accuracy: 0.9983 - loss: 0.0083 - val_accuracy: 0.9956 - val_loss: 0.0158\n",
      "Epoch 4/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 45ms/step - accuracy: 0.9954 - loss: 0.0176 - val_accuracy: 0.9979 - val_loss: 0.0100\n",
      "Epoch 5/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 53ms/step - accuracy: 0.9983 - loss: 0.0077 - val_accuracy: 0.9963 - val_loss: 0.0134\n",
      "Epoch 6/100\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 54ms/step - accuracy: 0.9977 - loss: 0.0104 - val_accuracy: 0.9971 - val_loss: 0.0116\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "--- Resultaten ---\n",
      "Laatste Training Accuracy: 99.84%\n",
      "Beste Validatie Accuracy: 99.71%\n"
     ]
    }
   ],
   "source": [
    "# Early Stopping Callback\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\",\n",
    "    min_delta=0.001,  # 0.1% verbetering\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Start de training\n",
    "history = model_no_mask.fit(\n",
    "    x=[X_train_enc, X_train_dec],\n",
    "    y=Y_train_dec,\n",
    "    validation_data=([X_valid_enc, X_valid_dec], Y_valid_dec),\n",
    "    epochs=100, # Maximaal aantal epochs (Early Stopping stopt eerder)\n",
    "    callbacks=[early_stopping_cb],\n",
    "    batch_size=64 # Een standaard batch size\n",
    ")\n",
    "\n",
    "# Controleer de uiteindelijke nauwkeurigheid\n",
    "train_accuracy = history.history['accuracy'][-1]\n",
    "val_accuracy = history.history['val_accuracy'][-1]\n",
    "\n",
    "print(\"\\n--- Resultaten ---\")\n",
    "print(f\"Laatste Training Accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Beste Validatie Accuracy: {val_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Het is te verwachten dat beide > 99% zullen zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0844ab15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximale lengte human-readable datum in training set: 29\n",
      "Maximale lengte machine-readable datum in training set: 11\n"
     ]
    }
   ],
   "source": [
    "max_len_human_readable = max(len(s) for s in X_train_enc)\n",
    "max_len_machine_readable = max(len(s) for s in Y_train_dec)\n",
    "print(f\"Maximale lengte human-readable datum in training set: {max_len_human_readable}\")\n",
    "print(f\"Maximale lengte machine-readable datum in training set: {max_len_machine_readable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "67956c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dec_inputs = keras.ops.convert_to_tensor([['.']])\n",
    "test_enc_inputs = keras.ops.convert_to_tensor([[\"13 nov 2024\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7a134a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 1, 13)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_enc = enc_text_vec_layer(test_enc_inputs)\n",
    "test_dec = dec_text_vec_layer(test_dec_inputs) - 2\n",
    "result = model_no_mask.predict([test_enc, test_dec])\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "66159033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 13), dtype=float32, numpy=\n",
       "array([[8.2934555e-03, 3.3951804e-04, 4.5100256e-07, 6.0762298e-01,\n",
       "        3.4895864e-01, 6.9088012e-04, 3.1868402e-02, 3.3596950e-04,\n",
       "        1.1660886e-05, 3.0571240e-04, 1.9308309e-05, 3.1411383e-04,\n",
       "        1.2389970e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.ops.softmax(result)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "792f466b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 13)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0f479efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = keras.ops.argmax(keras.ops.softmax(result[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e07ad4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "np.str_('1')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_text_vec_layer.get_vocabulary()[i+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "133ba084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 13)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
