{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZLgogzsXTk7"
      },
      "source": [
        "## Text Classification using an Encoder-Only Transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PD1vZlqP0aN"
      },
      "source": [
        "In this exercise we will build an encoder-only transformer and apply it on a text\n",
        "classification task. To better understand the model architecture, we will build and train the model (almost) from scratch.\n",
        "\n",
        "**Note**: in practice, you would use a pretrained language model and possibly finetune it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "4QMu0xYuRDSB"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wWygoMoaNIh"
      },
      "source": [
        "# Part 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixXLKowYQDrU"
      },
      "source": [
        "**2.1 Copy and Change Classes from Previous Exercise**\n",
        "\n",
        "An encoder-only transformer is very similar to a decoder-only transformer:\n",
        "\n",
        "-   It also starts with a positional embedding.  \n",
        "    We are going to use the code for `PositionalEmbedding` from the previous exercise.\n",
        "-   The encoder blocks employ the same `FeedForward` layer as the decoder blocks.  \n",
        "    We are going to use the code for `FeedForward` from the previous exercise.\n",
        "-   The encoder block itself is very similar to the decoder block in a decoder-only transformer.  \n",
        "    We are going to use the code for `GTPDecoderBlock` from the previous exercise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "BaGLk-rkRJTH"
      },
      "outputs": [],
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class FeedForward(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, factor=4, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.factor = factor\n",
        "        self.relu = keras.activations.relu\n",
        "\n",
        "    def build(self, batch_input_shape):\n",
        "        time_steps, embed_size = batch_input_shape[1:]\n",
        "        self.kernel1 = self.add_weight(\n",
        "            shape=(embed_size, self.factor*embed_size))\n",
        "        self.bias1 = self.add_weight(shape=(self.factor*embed_size, ),\n",
        "                                     initializer=\"zeros\")\n",
        "        self.kernel2 = self.add_weight(\n",
        "            shape=(self.factor*embed_size, embed_size))\n",
        "        self.bias2 = self.add_weight(shape=(embed_size, ),\n",
        "                                     initializer=\"zeros\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        a = self.relu(keras.ops.matmul(inputs, self.kernel1) + self.bias1)\n",
        "        return keras.ops.matmul(a, self.kernel2) + self.bias2\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {\n",
        "            **base_config,\n",
        "            \"factor\": self.factor,\n",
        "        }\n",
        "\n",
        "# Embedding with Position\n",
        "\n",
        "\n",
        "@keras.saving.register_keras_serializable()\n",
        "class EmbeddingWithPosition(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Computes and embedding and also adds a positional embedding.\n",
        "    This layer does not support masking.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_tokens, max_seq_length, embed_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_tokens = num_tokens\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "    def build(self, batch_input_shape):\n",
        "        # Shape not needed\n",
        "        # print(f\"Build called with {batch_input_shape} as shape\")\n",
        "\n",
        "        self.kernel = self.add_weight(shape=(self.num_tokens, self.embed_size))\n",
        "        self.pos_kernel = self.add_weight(\n",
        "            shape=(self.max_seq_length, self.embed_size))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        _, length = keras.ops.shape(inputs)\n",
        "\n",
        "        # (batch, length, embed_size)\n",
        "        embeddings = keras.ops.take(self.kernel, inputs, axis=0)\n",
        "        pos_embeddings = self.pos_kernel[:length]\n",
        "\n",
        "        return embeddings + pos_embeddings  # rely on broadcasting. Mask is lost\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {\n",
        "            **base_config,\n",
        "            \"num_tokens\": self.num_tokens,\n",
        "            \"max_seq_length\": self.max_seq_length,\n",
        "            \"embed_size\": self.embed_size\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vMGJSrhSTQC"
      },
      "source": [
        "Make the following changes to `GPTDecoderBlock`:\n",
        "\n",
        "-   Rename the class to `EncoderBlock`.\n",
        "-   The multi-headed attention does not need the causal mask when it is called, since each token is allowed to attend to all tokens before ánd after it. Remove the causal mask.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "lVpwP5HBRXKQ"
      },
      "outputs": [],
      "source": [
        "# A GPT decoder block (without cross attention)\n",
        "@keras.saving.register_keras_serializable()\n",
        "class GPTDecoderBlock(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, num_heads, embed_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_heads = num_heads\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "        self.masked_multi_head_attn = keras.layers.MultiHeadAttention(\n",
        "            num_heads=self.num_heads,\n",
        "            key_dim=self.embed_size // self.num_heads\n",
        "        )\n",
        "        self.layer_norm_1 = keras.layers.LayerNormalization()\n",
        "        self.feed_forward = FeedForward()\n",
        "        self.layer_norm_2 = keras.layers.LayerNormalization()\n",
        "\n",
        "    def build(self):\n",
        "        pass\n",
        "\n",
        "    def call(self, inputs):\n",
        "        skip = inputs\n",
        "        inputs = self.masked_multi_head_attn(\n",
        "            inputs, inputs, use_causal_mask=True)\n",
        "        inputs = self.layer_norm_1(keras.layers.Add()([inputs, skip]))\n",
        "\n",
        "        skip = inputs\n",
        "        inputs = self.feed_forward(inputs)\n",
        "        inputs = self.layer_norm_2(keras.layers.Add()([skip, inputs]))\n",
        "        return inputs\n",
        "\n",
        "    def get_config(self):\n",
        "        base_config = super().get_config()\n",
        "        return {\n",
        "            **base_config,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"embed_size\": self.embed_size\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6S9UG-RTmvI"
      },
      "source": [
        "**2.2 Create an Encoder-Only Classification Model**  \n",
        "The encoder from the original \"Attention is all you Need\" paper outputs contextual\n",
        "embeddings of shape: (`batch_size`, `seq_length`, `embed_size`).  \n",
        "**Note**: in principle this model will work with any sequence length, as\n",
        "long as it is not longer than the maximum sequence length as specified in `PositionalEmbedding`.\n",
        "We need a way of attaching a small network for classification on top of these contextual embeddings. We will do the following (other options are certainly possible):\n",
        "\n",
        "-   First, we will reduce the contextual embeddings of shape\n",
        "    (`batch_size`, `seq_length`, `embed_size`) to a tensor of fixed size by\n",
        "    viewing these contextual embeddings as a one-dimensional sequence and applying a one-dimensional global average pooling layer to it. This reduces the\n",
        "    contextual embeddings to a tensor of size (`batch_size`, `embed_size`).\n",
        "-   Next, we add a fully connected layer with `embed_size` units and the ReLU\n",
        "    activation function.\n",
        "-   Finally, we add a fully connected layer with the correct number of units and\n",
        "    the correct activation function given the type of classification problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "A8CN57bIRotx"
      },
      "outputs": [],
      "source": [
        "def get_classification_encoder_model(\n",
        "    num_tokens: int, max_seq_length: int,\n",
        "    embed_size: int, num_heads:int, num_blocks:int,\n",
        "    num_classes:int,\n",
        "    use_mask=False,\n",
        "    scale_embeddings=False):\n",
        "    \"\"\"\n",
        "    num_tokens: the vocabulary size\n",
        "    max_seq_length: maximum length of any sequence\n",
        "    embed_size: the embedding dimension\n",
        "    num_heads: the number of heads in each multi-headed attention\n",
        "    num_blocks: the number of encoder blocks\n",
        "    num_classes: the number of classes to classify the text into\n",
        "    \"\"\"\n",
        "    inputs = keras.layers.Input(shape=[max_seq_length], dtype=int) # (B, LEN)\n",
        "\n",
        "    # Positional Embeddings\n",
        "    # YOUR CODE HERE\n",
        "    x = EmbeddingWithPosition(\n",
        "        num_tokens=num_tokens,\n",
        "        max_seq_length=max_seq_length,\n",
        "        embed_size=embed_size\n",
        "    )(inputs)  # (B, LEN, EMBED_SIZE)\n",
        "\n",
        "    # Encoder blocks\n",
        "    # YOUR CODE HERE\n",
        "    for _ in range(num_blocks):\n",
        "        x = GPTDecoderBlock(\n",
        "            num_heads=num_heads,\n",
        "            embed_size=embed_size\n",
        "        )(x)\n",
        "\n",
        "    # Simplest classification head\n",
        "    # Classification network on top of contextual embeddings\n",
        "    # YOUR CODE HERE\n",
        "    x = keras.layers.GlobalAveragePooling1D()(x)\n",
        "    x = keras.layers.Dense(embed_size, activation='relu')(x)\n",
        "\n",
        "    if num_classes == 2:\n",
        "        output = keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "    else:\n",
        "        output = keras.layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clCSau2JSBYw"
      },
      "source": [
        "## Load Data\n",
        "\n",
        "**2.3 Apply the Model to the IMDB-Dataset**  \n",
        "We will now apply this model to the built-in IMDB-dataset. Load the data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "IIJt0p_GSDKh"
      },
      "outputs": [],
      "source": [
        "MAX_LEN = 500\n",
        "NUM_TOKENS = 10_000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "zqMT6LFBSJkg"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(\n",
        "    num_words=NUM_TOKENS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecpdg4UIUtyH"
      },
      "source": [
        "Not all reviews contain the same number of words. Use a method from\n",
        "`keras.utils` to pad the reviews so that they all have the same length. Use arguments so that\n",
        "\n",
        "-   padding tokens are added at the end of reviews that are shorter than `MAX_LEN`.\n",
        "-   reviews that are longer than `MAX_LEN` should be truncated so that the start of the review is kept.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "OdXkoX5CSN3X"
      },
      "outputs": [],
      "source": [
        "# pad sequences\n",
        "X_train = keras.utils.pad_sequences(\n",
        "    X_train,\n",
        "    maxlen=MAX_LEN,\n",
        "    padding='post',\n",
        "    truncating='post'\n",
        ")\n",
        "X_test = keras.utils.pad_sequences(\n",
        "    X_test,\n",
        "    maxlen=MAX_LEN,\n",
        "    padding='post',\n",
        "    truncating='post'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "0Sa3UwtzSSm_"
      },
      "outputs": [],
      "source": [
        "X_val = X_test[:10000]\n",
        "y_val = y_test[:10000]\n",
        "\n",
        "X_test = X_test[10000:]\n",
        "y_test = y_test[10000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTtZo-ChVJPe"
      },
      "source": [
        "Now that the data is ready, we can build a model. Use the following parameters:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Aj8oJUM9ST9z"
      },
      "outputs": [],
      "source": [
        "NUM_HEADS = 4\n",
        "EMBED_SIZE = 32\n",
        "NUM_BLOCKS = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "1NtsgUK4Sd7g"
      },
      "outputs": [],
      "source": [
        "model = get_classification_encoder_model(\n",
        "    num_tokens=NUM_TOKENS,\n",
        "    max_seq_length=MAX_LEN,\n",
        "    embed_size=EMBED_SIZE,\n",
        "    num_heads=NUM_HEADS,\n",
        "    num_blocks=NUM_BLOCKS,\n",
        "    num_classes=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "namv3gC8VPz-"
      },
      "source": [
        "How many parameters does the model have? You should find that this model has 362 497 parameters, most of which are in the embedding layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "BNER_qHrSiD6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_with_position_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">336,000</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">EmbeddingWithPosition</span>)         │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gpt_decoder_block_6             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,704</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPTDecoderBlock</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gpt_decoder_block_7             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">500</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,704</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GPTDecoderBlock</span>)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_3      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_with_position_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │       \u001b[38;5;34m336,000\u001b[0m │\n",
              "│ (\u001b[38;5;33mEmbeddingWithPosition\u001b[0m)         │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gpt_decoder_block_6             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m12,704\u001b[0m │\n",
              "│ (\u001b[38;5;33mGPTDecoderBlock\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ gpt_decoder_block_7             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m500\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │        \u001b[38;5;34m12,704\u001b[0m │\n",
              "│ (\u001b[38;5;33mGPTDecoderBlock\u001b[0m)               │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_average_pooling1d_3      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m1,056\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">362,497</span> (1.38 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m362,497\u001b[0m (1.38 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">362,497</span> (1.38 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m362,497\u001b[0m (1.38 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6bKn_5jVa_9"
      },
      "source": [
        "Compile and fit the model:\n",
        "\n",
        "-   Use the `Adam` optimizer with all the default values.\n",
        "-   Use early stopping to (try to) prevent (severe) overfitting. Monitor the validation accuracy, using a `patience` of 4. Restore the best weights.\n",
        "\n",
        "When you train the model, you should find that it reaches a validation accuracy of around 86%, while the accuracy on the training data is much higher, so the model is definitely overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "C2Chzj6TSwwj"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 2\n",
        "if NUM_CLASSES == 2:\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "else:\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "2T-oY-pAVnF-"
      },
      "outputs": [],
      "source": [
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=4,\n",
        "    restore_best_weights=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "jgBWk7_qS4RV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/buraq-mac/Documents/School/CollegeFiles/DeepLearning/Deep_Learning_Exercises/.venv/lib/python3.12/site-packages/keras/src/ops/nn.py:946: UserWarning: You are using a softmax over axis -1 of a tensor of shape (None, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "2025-12-07 15:28:18.999785: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 324ms/step - accuracy: 0.5000 - loss: 0.4095 - val_accuracy: 0.4973 - val_loss: 0.2996\n",
            "Epoch 2/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m298s\u001b[0m 381ms/step - accuracy: 0.5000 - loss: 0.2095 - val_accuracy: 0.4973 - val_loss: 0.3338\n",
            "Epoch 3/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m295s\u001b[0m 377ms/step - accuracy: 0.5000 - loss: 0.1377 - val_accuracy: 0.4973 - val_loss: 0.4153\n",
            "Epoch 4/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m307s\u001b[0m 393ms/step - accuracy: 0.5000 - loss: 0.0891 - val_accuracy: 0.4973 - val_loss: 0.5163\n",
            "Epoch 5/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 413ms/step - accuracy: 0.5000 - loss: 0.0606 - val_accuracy: 0.4973 - val_loss: 0.5623\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deep-learning-exercises",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
