{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e207f5e1",
   "metadata": {},
   "source": [
    "# Praktische vragen Deep Learning\n",
    "\n",
    "Sabine De Vreese, Stijn Lievens en Simon De Gheselle\n",
    "\n",
    "17 November 2025\n",
    "\n",
    "# 1  Generating (Shakespearean) Text with a GPT-like Transformer\n",
    "In this exercise we are going to build a GPT-like transformer. Such a transformeris a decoder-only transformer and hence the doesnâ€™t include a cross attention layer.\n",
    "\n",
    "As a practical application of the transformer, we will train it to generate Shakespearean text.\n",
    "\n",
    "Note: the following video1from Andrej Karpathy explains in detail how to builda GPT-like decoder. In the video the Pytorch framework is used, but the con-cepts are identical. The video also follows the â€œAttention is all you needâ€ paper,apart from the placement of the `LayerNormalization` layers.The overall architecture of adecoder-only transformeris shown in Figure 1\n",
    "\n",
    "## 1.1  Implement theFeedForwardLayer\n",
    "We start by implementing the FeedForward layer. According to equation (2) ofthe â€œAttention is all you Needâ€ paper, this layer performs the following calcula-tion:\n",
    "$$\n",
    "FFN(ð‘¥) =max(0, ð‘¥ð‘Š_1+ ð‘_1)ð‘Š_2+ ð‘_2\n",
    "$$\n",
    "which is applied to each position (i.e. each time step)ð‘¥independently. Thedimensions of the input and output are identical, but the layer in between is4times wider.For this exercise, we are going to implement this layer as a subclass ofkeras.layers.Layerbut we willnotuse any other layers, instead you should"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b221bc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as ks\n",
    "# SimpleFeedForwardlayer\n",
    "@ks.saving.register_keras_serializable()\n",
    "class FeedForward(ks.layers.Layer):\n",
    "    def __init__(self, factor=4, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.factor = factor\n",
    "    def build(self, batch_input_shape):\n",
    "        time_steps, embed_size = batch_input_shape[1:]\n",
    "        #* YOUR CODE HERE:\n",
    "        # Add weights and biases here\n",
    "\n",
    "    def call(self, inputs):\n",
    "        #* YOUR CODE HERE:\n",
    "        # Perform calculation\n",
    "        pass\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return{**base_config,\"factor\": self.factor,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cdcf5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-04 16:25:03.178318: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4\n",
      "2026-01-04 16:25:03.178662: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2026-01-04 16:25:03.178707: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2026-01-04 16:25:03.179173: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2026-01-04 16:25:03.179193: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Simpletestcode\n",
    "TEST_SHAPE = (2, 10, 32)  # Batchsize2,10timesteps,embeddingdimension32\n",
    "X = ks.random.normal(shape=TEST_SHAPE)\n",
    "ff = FeedForward()\n",
    "# Shouldprint(2,10,32)\n",
    "for w in ff.get_weights():  # Checkthattheshapesarewhatyouexpectprint(w.shape)\n",
    "    print(f\"Shape of output{ff(X).shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
